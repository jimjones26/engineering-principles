This is a love letter to engineering. I firmly believe that AI makes engineering more essential, not less. I am going to explain why in detail, because I think that people who are not engineers do not understand this. I also think that increasingly, junior engineers are afraid, because they have not experienced in detail what it is like to work with senior engineers at scale. I have experienced this. I have worked with senior engineers. I work with principal engineers. I know what it feels like to have a very strong engineering mind, or a collection of minds, in the room reviewing a technical specification. I want to explain in this video why I am highly convinced that engineering is not going anywhere as a discipline. In fact, I will go further: I will say that engineering is more important now than it was before the age of AI. The fear is real, but it is backwards. Yes, absolutely, boilerplate code can be generated automatically. I do not doubt that; I see it all the time. Yes, AI can write working code from natural language. That is true. However, working code and engineered systems are worlds apart. They are not close to the same thing. In fact, most people are finding that out as they develop code systems that may be ready to launch to friends and family, but are not ready for actual production. I would also argue that one of the reasons why engineering matters more now is precisely because AI can write code. The blast radius of AI-generated failures is exponentially higher when AI can write the code for you. Therefore, we are not being replaced. Engineers are not in danger of being replaced, because engineers are being asked to take positions of greater responsibility over AI, with AI, in partnership with AI—and we will get into that. However, I want to lay that out as a contention, because I think we just need to say it out loud. My position is that yes, there will be some engineers who do not understand how engineering works that will absolutely lose their roles in the age of AI, and frankly, many of them probably would have lost their roles previously. One of the interesting things when you work with a lot of different engineers, as I have, is that you realize how variable the talent mix is across engineers. An engineer at the same level can be worlds apart in terms of actual capability and impact to the business. Anyone who has worked with engineers will tell you the same thing. I had an intern when I worked at Amazon who did more work and delivered more value than senior engineers I knew there. It just was the way it was. He was motivated. He had a great assignment. He was able to get something into production, and he did a great job. Needless to say, he got an offer, right? The point is that talent is variable. Talent has always been variable. We should not mistake the fact that engineering is hard and talent is variable from the impact that AI is having on the engineering discipline. There is absolutely an impact on the engineering discipline from AI, and I am going to get into it in the rest of this video. However, it is not as simplistic as saying AI equals bad for engineering, which is what I see mostly. I am tired of it, and so that is why I am basically making a video as a love letter to engineering. So, let us bite off the first piece. I talked about AI-generated code. Let us talk about the vibe coding piece of this, right? People talk about vibe coding as replacing engineering. Now, anybody can speak an intent into lovable.dev, and they can get a working software back—or at least that is the idea. AI tools, I would argue, create a multiplier effect for trained engineers more than they create democratization of code. I know they democratize code. I know people who have never coded before who were able to do some coding. Now, engineers can do even more. Engineers can compress their expertise to carry architectural intent much more easily than non-engineers, because they understand the underlying technical systems. Non-engineers, frankly, when they get the chatbot and they get the ability to vibe code, I have seen them build stuff, but I have also seen them very frequently get just enough rope to hang themselves. Engineers understand those limitations that coding brings. They understand how to read code. They understand how the system components work together, and they are able to go faster as a result. If the non-engineers get rope to hang themselves, the engineers get rocket fuel. So, the digital divide is shifting very rapidly from who can code to who can engineer. I want to pause there, because I actually do not believe that only people from conventional computer science backgrounds can engineer. If you understand how software components go together, that is increasingly the essential skill. If you can engineer systems so they are efficient, if you understand how backend works with frontend, if you understand what attack surfaces look like from a security perspective, if you understand how to move software into production and what that requirement looks like, those are not exclusively skills you learn in computer science. In fact, many engineers will tell you they did not learn them in their computer science majors. They learned classical computer science. Engineering is something we have frequently learned on the job. However, it is not something limited only to software engineers. A lot of people can learn engineering principles. What I have observed is that part of the reason why I am saying software engineers go faster with vibe coding is because they have already inculcated into themselves—they have absorbed into themselves—these principles of engineering. So, they feel native, and that is really the differentiator. It is not that they have a computer science degree. It is not that they know every single bit of JavaScript or TypeScript or whatever it is. It is that they know how to engineer. That is encouraging, because it means if you are trying to also learn how to build efficiently in the age of AI, you can do so faster by just learning some of the skills of engineering. I am going to lay out what I view as some of the new core skills for engineering based on lots of work with engineers through this AI transition. So, the thing I want to leave you with as we move on from sort of the AI coding piece and get into some of the other parts of this engineering domain that we are exploring in this video is that effective prompting is an engineering skill. I have taught courses on effective prompting. Increasingly, I think that this is a truth that we do not admit: effective prompting is an engineering skill that requires some degree of engineering understanding. The more engineering understanding you have, the more effective your prompting is going to be. All right, let us go from just sort of that sense of vibe coding and getting rid of the fear of engineering into the human responsibilities that I do not think change and that I think still sit with engineers. Now, some of these will sit with engineers more in scaled systems, like at Amazon or at Google. Some will still work for smaller teams as well. However, I wanted to call out the human responsibilities here, because I think that we spend a lot of time talking about AI responsibilities and not a lot of time talking about the human component. The human component is, I would argue, getting more important as we multiply our code by AI. Number one, it is a human responsibility to translate intent to correct specification. So, name the invariances, name the hazards, name the success criteria, translate human needs into system edges and boundaries, decide not just whether we can do something but whether we should do something. You are carrying the weight of systems that, if you are working at a large company, can affect billions of people. Translating intent to specification implies a degree of skin in the game that AI systems do not have. The second one I want to call out is that humans are responsible for writing guarantees on probabilistic systems. AI is behaviorally, at scale, a functional probabilistic system. It is not always X or Y; it is a probability at scale. You are turning likelihood into contracts if you are engineering systems. You have to be able to guarantee outcomes. You have to be able to guarantee edges. You have to be able to guarantee security to some degree. Fundamentally, a lot of the human job is taking these probabilistic systems and writing contracts against them that you can uphold. So, you have to be able to create boundaries that are deterministic, not probabilistic. You have to define probability budgets that work at scale across pipelines. You have to ensure that what must never happen really does not ever happen. I have talked about scale a lot, but this is true even at a small scale. ChatGPT will not give you the same response if you give it the same prompt. There will be subtle differences. Your job as an engineer—the role of engineering—is to ensure that that kind of variance is not toxic to the system. It is also a human responsibility to think at scale. You have to understand emergent behaviors when you scale up to a very large footprint. This one, I think, is specific to big companies, but understanding emergent behaviors at 100 million users is its own skill set. It is a human skill set that very few humans have. Knowing when algorithms become bottlenecks and where they bottleneck is a human skill, and it gets at something that is essentially a risk with AI. AI does much better at writing code than deleting code. One of the things that you see with really good engineers at scale is they know what they can remove. Being able to intuit how the system works at scale, being able to intuit where phase transitions from stable to chaotic occur in complicated systems—I have seen principal engineers do that. It is a remarkable skill. It is a human skill, and it means that they understand how to effectively deal with a world where one-in-a-billion events are actually things that happen on a regular basis because of the trillions of events that they are processing. I do not want to terrify you if you are an engineer who has not worked at that scale. You will notice that this is just one of a very large array of skills that I am talking about in engineering. My intent here is not to convey that only those engineers who work at 100 million users scale will survive. Instead, I am trying to call out that that is one aspect of engineering that remains very human, even if AI increasingly assists in helping us understand these systems. The last human skill that I want to call out is economic engineering. So, you have to be able to manage intelligence like a utility. You have to be able to optimize latency, quality, and cost through trade-offs. You have to be able to design degraded experiences that prioritize value even with additional constraints. You have to be able to understand where inefficiency matters and how it impacts margins. You have to engineer systems, especially in an age when tokens are intelligent and tokens cost money. How do you deliver intelligence cost-effectively? That is a human skill, and that is a skill that is scale-invariant. You should care about that even at a small scale. Now, I have talked about just a few; there are more human responsibilities, but as we are touring across the engineering domain that I love so much, I want to talk about some of the new disciplines that are emerging, because it is absolutely true that engineering is changing, and I do not want to pretend it is not. Therefore, I am going to suggest for you a few of the ways that we see engineering starting to shift in the age of AI, and then we will come back to the human skills and kind of revisit them after we look at that. So, semantic engineering—that is a new discipline, right? How do you debug meaning flow, not just data flow? How do you build semantic firewalls against injection attacks? I saw a new injection attack just today where someone can use the name field in ChatGPT to prompt inject something. Injection attacks come in all shapes and sizes. People are able to put injection attacks in white text on white on Reddit boards now, because the system cannot distinguish between your prompt and the context content it is reading. It is up to engineers to figure out how to address this stuff. It is up to engineers to design systems that will appropriately refuse to act. No, it is not just model makers; engineers installing these systems have the accountability to act as well. Boundary engineering: engineers have to architect the space between the probabilistic world of the LLM and the deterministic world that we expect with software. They have to create interfaces that feel consistent. Yes, I am going to go out on a limb and say not all interfaces are going to be created by AI on the fly. I do not think that is true. They have to be able to maintain human-AI boundaries in ways that preserve human agency. Increasingly, part of the engineering responsibility is figuring out how to map that boundary between human and LLM collaboration in software at scale. Memory and knowledge engineering is another one. How do you build institutional memory for AI system failures? How do you version data and prompts—even model weights—with rigor? How do you manage context windows economically? How do you build semantic forensics? How do you do debugging on a system that is in production and you could not fully debug it prior? That gets into safety and assurance engineering. How do you create live evaluation cultures? How do you build safety cases that have explicit maps between hazards and mitigations and evidence chains for audit? How do you design for hostile inputs as an assumption? How do you show what the system thought when the system is probabilistic? These are new skills for a reason. We do not fully have the answers here, but today's engineers are being tasked with using that core engineering skill set I talked about to attack these kinds of problems in the age of AI. So, let us revisit and ask ourselves, in that world with these kinds of new engineering skills emerging, what human skills really stand out. We talked about some initially. We talked about the importance of intent to specification. We talked about probabilistic systems and how you write guarantees against them, about thinking at scale, about economic engineering. There are some other human skills that I want to call out here that are going to be useful regardless of scale and regardless of where you are across these new engineering disciplines. I wanted to give you a flavor of what is new, and then we are going to come back here to what stays the same. System intuition stays the same. Good engineers sense bottlenecks. They sense problems to solve. They recognize emergent failures. Empathy is an engineering skill, because empathy requires you to bridge between the precision that machines require and the ambiguity that humans deal with. Effectively, that is what we are all doing in the age of AI. Empathy requires you to understand how millions of users will misuse your API. It requires understanding how to build systems that account for human nature. Judgment under uncertainty—that is another engineering skill. It requires you to make expensive decisions on very incomplete information. It requires you to know when good enough beats perfect, which, by the way, is one of those distinguishing characteristics of really good senior engineers. It requires you to choose appropriate trade-offs within constraints, to decide when randomness is helpful versus when it is unhelpful. Another human skill is the orchestration of complexity. You have to be able to coordinate tool chains, to conduct symphonies of intelligence involving multiple LLMs that actually work and deliver value. You have to be able to manage distributed systems where the components do not have pre-written contracts increasingly. You have to understand that semantic composability does not follow traditional rules of software engineering, and you are going to have to help create them. There is a lot of complexity to orchestrate. Now, we have always had to orchestrate complexity from the engineering perspective. It gets harder now. Why? Why am I writing this? Why does all of this matter? The truth is, if we did not have engineers, we would be in real trouble. The stakes have never been higher. AI makes it so trivial to ship failure at scale. As I said, systems will now accept paragraphs of instructions from the open internet. The attack vectors are so high. Model rot can corrupt systems without any warning at all. The reality is that we have to recognize that engineering is what enables us to take this wild world where LLMs can speak language, where they are probabilistic, and where they can bring intelligence to bear. Engineers help wrestle that into an operational, stable production system. They help to bring observability to those systems. They help to debug those systems. They help to figure out the energy and compute footprint that is appropriate for those systems. Engineers ultimately are cultural architects. They help us to design workflows that preserve human judgment. They help us to build interfaces that make AI reasoning inspectable. They help us to prevent automation bias and skill atrophy if they are designing systems well. Ultimately, they help us maintain dignity, because they can build systems that have to admit ignorance. Engineers have more responsibility now, not less. That is one of the takeaways that I want you to sit with. I am going to close with what I would suggest are three new laws of engineering in the age of AI. I chose these three for a reason. Number one: if you cannot write what is invariant, then you have not engineered the system. This captures the fundamental difference between vibe coding and engineering. In the age of AI, you have to be able to understand that an LLM will give you likelihood, not correctness. So, an invariant—something that does not change—is what separates engineering from gambling, which a lot of vibe coders are doing. It makes you think what properties survive when the probabilistic components do unexpected things. It forces you to do resilience engineering. It is the difference between "make it work" and "define what working means," between "it seems right" and "here is what must always be true." I would also say that the second law of engineering, for lack of a better term: if you cannot measure it in production, then you did not really build it. This requires you to go beyond the demo culture that AI enables. It is really easy to generate prototypes now; AI makes demos almost free, but production is different. Production means real users doing really weird things. It means scale effects. It means edge cases. It means model drift. Engineering insists on observability, telemetry, semantic forensics. You cannot just ship code that worked once in a workbook. We have always insisted on production as a bar for engineering. It is harder to hit now, and so that is why I am reiterating it in this second law of engineering. The third one—the third law: if you cannot explain why it failed, you have not owned the system. Again, we have emphasized accountability with engineering, but it is more important now. AI systems are entering regulated spaces—spaces where they have to be able to explain what happened. Human responsibility requires us to own the explanation, the accountability, and where the buck stops. If you cannot explain what happened in your system to a very smart non-engineer, then you probably do not really understand your own system, and you probably did not really engineer it. These three laws are actually designed to fit together. They are designed to be three pieces of the engineering life cycle—the new engineering life cycle in the age of AI. Number one is specification: what we promise when we build a system, how we write contracts that stick regardless of probabilistic systems. That is the invariant piece. Number two is verification or measurement: how we prove that we delivered something in production. Number three is accountability or explanations: how you take ownership of outcomes. Ultimately, the engineers that succeed are going to be engineers who think before they build, who validate in production, and who own the consequences. You know what? That is not a new skill. That is part of why I created this video, circling all the way back to the beginning. This is a love letter to engineering, because even though engineering is evolving in the age of AI—and I hope I have given you a sense of that here—engineering principles are remarkably constant. The need to design systems that work is not changing. If you walk away with anything, I want you to walk away with the recognition that computing requires engineering. Engineering is not going out of style. If anything, the increased complexity of computing—the 100x, the thousandx complexity of computing that we get in the age of AI—is going to increase the need for skilled engineers. So, there you go. That is why I think engineers are not going anywhere. That is why I think we need to appreciate them more.
